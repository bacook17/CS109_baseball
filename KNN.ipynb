{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matchupdf = pd.read_csv(\"temp_matchupdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>H</th>\n",
       "      <th>PA</th>\n",
       "      <th>RL</th>\n",
       "      <th>SAC</th>\n",
       "      <th>SO</th>\n",
       "      <th>TB</th>\n",
       "      <th>W</th>\n",
       "      <th>bID</th>\n",
       "      <th>matchID</th>\n",
       "      <th>pID</th>\n",
       "      <th>AVG</th>\n",
       "      <th>OBP</th>\n",
       "      <th>ov_AB</th>\n",
       "      <th>ov_H</th>\n",
       "      <th>ov_PA</th>\n",
       "      <th>ov_SAC</th>\n",
       "      <th>ov_SO</th>\n",
       "      <th>ov_TB</th>\n",
       "      <th>ov_W</th>\n",
       "      <th>ov_FACED</th>\n",
       "      <th>ov_AVG</th>\n",
       "      <th>ov_OBP</th>\n",
       "      <th>ov_SO_PCT</th>\n",
       "      <th>ov_W_PCT</th>\n",
       "      <th>ov_H_PCT</th>\n",
       "      <th>ovp_AB</th>\n",
       "      <th>ovp_H</th>\n",
       "      <th>ovp_PA</th>\n",
       "      <th>ovp_SAC</th>\n",
       "      <th>ovp_SO</th>\n",
       "      <th>ovp_TB</th>\n",
       "      <th>ovp_W</th>\n",
       "      <th>ovp_FACED</th>\n",
       "      <th>ovp_SO_PCT</th>\n",
       "      <th>ovp_W_PCT</th>\n",
       "      <th>ovp_H_PCT</th>\n",
       "      <th>ovp_AVG</th>\n",
       "      <th>ovp_OBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>torrc001</td>\n",
       "      <td>torrc001_medlk001</td>\n",
       "      <td>medlk001</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1405</td>\n",
       "      <td>357</td>\n",
       "      <td>1502</td>\n",
       "      <td>13</td>\n",
       "      <td>297</td>\n",
       "      <td>674</td>\n",
       "      <td>97</td>\n",
       "      <td>188</td>\n",
       "      <td>0.197736</td>\n",
       "      <td>0.064581</td>\n",
       "      <td>0.237683</td>\n",
       "      <td>0.254093</td>\n",
       "      <td>0.302264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kinnm002</td>\n",
       "      <td>kinnm002_leita001</td>\n",
       "      <td>leita001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5014</td>\n",
       "      <td>1260</td>\n",
       "      <td>5776</td>\n",
       "      <td>91</td>\n",
       "      <td>1023</td>\n",
       "      <td>2335</td>\n",
       "      <td>762</td>\n",
       "      <td>356</td>\n",
       "      <td>0.177112</td>\n",
       "      <td>0.131925</td>\n",
       "      <td>0.218144</td>\n",
       "      <td>0.251296</td>\n",
       "      <td>0.350069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>telit001</td>\n",
       "      <td>telit001_felds001</td>\n",
       "      <td>felds001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3602</td>\n",
       "      <td>979</td>\n",
       "      <td>3985</td>\n",
       "      <td>53</td>\n",
       "      <td>556</td>\n",
       "      <td>1835</td>\n",
       "      <td>383</td>\n",
       "      <td>347</td>\n",
       "      <td>0.139523</td>\n",
       "      <td>0.096110</td>\n",
       "      <td>0.245671</td>\n",
       "      <td>0.271793</td>\n",
       "      <td>0.341782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>telit001</td>\n",
       "      <td>telit001_grays001</td>\n",
       "      <td>grays001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>793</td>\n",
       "      <td>177</td>\n",
       "      <td>864</td>\n",
       "      <td>9</td>\n",
       "      <td>189</td>\n",
       "      <td>289</td>\n",
       "      <td>71</td>\n",
       "      <td>110</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.082176</td>\n",
       "      <td>0.204861</td>\n",
       "      <td>0.223203</td>\n",
       "      <td>0.287037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>R</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>schic002</td>\n",
       "      <td>schic002_adamt001</td>\n",
       "      <td>adamt001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>411</td>\n",
       "      <td>66</td>\n",
       "      <td>424</td>\n",
       "      <td>62</td>\n",
       "      <td>143</td>\n",
       "      <td>84</td>\n",
       "      <td>13</td>\n",
       "      <td>72</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.186321</td>\n",
       "      <td>0.337264</td>\n",
       "      <td>0.03066</td>\n",
       "      <td>0.155660</td>\n",
       "      <td>2076</td>\n",
       "      <td>566</td>\n",
       "      <td>2346</td>\n",
       "      <td>34</td>\n",
       "      <td>399</td>\n",
       "      <td>937</td>\n",
       "      <td>270</td>\n",
       "      <td>243</td>\n",
       "      <td>0.170077</td>\n",
       "      <td>0.115090</td>\n",
       "      <td>0.241262</td>\n",
       "      <td>0.272640</td>\n",
       "      <td>0.356351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AB  H  PA RL  SAC  SO  TB  W       bID            matchID       pID       AVG       OBP  ov_AB  ov_H  ov_PA  ov_SAC  ov_SO  ov_TB  ov_W  ov_FACED    ov_AVG    ov_OBP  ov_SO_PCT  ov_W_PCT  ov_H_PCT  ovp_AB  ovp_H  ovp_PA  ovp_SAC  ovp_SO  ovp_TB  ovp_W  ovp_FACED  ovp_SO_PCT  ovp_W_PCT  ovp_H_PCT   ovp_AVG   ovp_OBP\n",
       "0   5  1   5  R    0   2   1  0  torrc001  torrc001_medlk001  medlk001  0.200000  0.200000      5     1      5       0      2      1     0         1  0.200000  0.200000   0.400000   0.00000  0.200000    1405    357    1502       13     297     674     97        188    0.197736   0.064581   0.237683  0.254093  0.302264\n",
       "1   3  0   3  L    1   2   0  0  kinnm002  kinnm002_leita001  leita001  0.000000  0.000000      3     0      3       1      2      0     0         1  0.000000  0.000000   0.666667   0.00000  0.000000    5014   1260    5776       91    1023    2335    762        356    0.177112   0.131925   0.218144  0.251296  0.350069\n",
       "2   6  0   6  R    0   0   0  0  telit001  telit001_felds001  felds001  0.000000  0.000000     12     2     12       0      2      4     0         2  0.166667  0.166667   0.166667   0.00000  0.166667    3602    979    3985       53     556    1835    383        347    0.139523   0.096110   0.245671  0.271793  0.341782\n",
       "3   6  2   6  R    0   2   4  0  telit001  telit001_grays001  grays001  0.333333  0.333333     12     2     12       0      2      4     0         2  0.166667  0.166667   0.166667   0.00000  0.166667     793    177     864        9     189     289     71        110    0.218750   0.082176   0.204861  0.223203  0.287037\n",
       "4   3  0   4  R    2   3   0  1  schic002  schic002_adamt001  adamt001  0.000000  0.250000    411    66    424      62    143     84    13        72  0.160584  0.186321   0.337264   0.03066  0.155660    2076    566    2346       34     399     937    270        243    0.170077   0.115090   0.241262  0.272640  0.356351"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchupdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_supports(df):\n",
    "    ubids=df.bID.unique()\n",
    "    pitch = df.groupby('pID').bID.unique()\n",
    "    bdict={}\n",
    "    for e,v in zip(pitch.index.values, pitch.values):\n",
    "        bdict[e] = np.array([item in v for item in ubids])\n",
    "    pitchers=bdict.keys()\n",
    "    supports=[]\n",
    "    for i,p1 in enumerate(pitchers):\n",
    "        for j,p2 in enumerate(pitchers):\n",
    "            if  i < j:\n",
    "                supmask = (bdict[p1] & bdict[p2])\n",
    "                common_batters = np.sum(supmask)\n",
    "                supports.append(common_batters)\n",
    "    print \"mean support\",np.mean(supports), \"median support\", np.median(supports)\n",
    "    return supports, bdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recompute_frame(ldf):\n",
    "    \"\"\"\n",
    "    takes a dataframe ldf, makes a copy of it, and returns the copy\n",
    "    with all conglomerations recomputed\n",
    "    this is used when a frame is subsetted.\n",
    "    \"\"\"\n",
    "    ldfb=ldf.groupby('bID')\n",
    "    ldfp=ldf.groupby('pID')\n",
    "    nldf=ldf.copy()\n",
    "    #Conglomerate pitcher stats\n",
    "    nldf.set_index(['pID'], inplace=True)\n",
    "    for col in ['AB', 'PA', 'H', 'TB', 'SAC', 'SO', 'W']:\n",
    "        nldf['ovp_'+col] = ldfp[col].sum()\n",
    "    nldf['ovp_AVG'] = nldf['ovp_H']/nldf['ovp_AB']\n",
    "    nldf['ovp_FACED']= ldfp.AB.count()\n",
    "    nldf['ovp_OBP'] = (nldf['ovp_H'] + nldf['ovp_W'])/nldf['ovp_PA']\n",
    "    for col in ['SO', 'W', 'H']:\n",
    "        nldf['ovp_' + col + '_PCT'] = nldf['ovp_' + col] / nldf['ovp_PA']\n",
    "    nldf.reset_index(inplace=True)\n",
    "    \n",
    "    #Conglomerate batter stats\n",
    "    nldf.set_index(['bID'], inplace=True)\n",
    "    for col in ['AB', 'PA', 'H', 'TB', 'SAC', 'SO', 'W']:\n",
    "        nldf['ov_'+col] = ldfb[col].sum()\n",
    "    nldf['ov_AVG'] = nldf['ov_H']/nldf['ov_AB']\n",
    "    nldf['ov_FACED']= ldfb.AB.count()\n",
    "    nldf['ov_OBP'] = (nldf['ov_H'] + nldf['ov_W'])/nldf['ov_PA']\n",
    "    for col in ['SO', 'W', 'H']:\n",
    "        nldf['ov_' + col + '_PCT'] = nldf['ov_' + col] / nldf['ov_PA']\n",
    "    nldf.reset_index(inplace=True)\n",
    "    return nldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_smaller(df, pacountp, pacountb):\n",
    "    smallidf1=df[(df.ovp_PA > pacountp) & (df.ov_PA > pacountb)]\n",
    "    smalldf=recompute_frame(smallidf1)\n",
    "    return smalldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_smaller_2(df, col, limit, greater=True):\n",
    "    if greater:\n",
    "        smallidf = df[(df[col] > limit)]\n",
    "    else:\n",
    "        smallidf = df[(df[col] < limit)]\n",
    "    return recompute_frame(smallidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smalldf[smalldf.user_review_count > 12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainlist=[]\n",
    "testlist=[]\n",
    "validatelist=[]\n",
    "take=5\n",
    "for k, v in smalldf.groupby('user_id'):\n",
    "    if np.mean(v.user_review_count) > 12:\n",
    "        takenos=np.random.choice(range(take), size=take/2, replace=False)\n",
    "        takelist=np.array([e in takenos for e in range(take)])\n",
    "        validatelist.append(v[-take:][~takelist])#use those \n",
    "        testlist.append(v[-take:][takelist])#use the other \n",
    "        trainlist.append(v[:-take])\n",
    "    else:\n",
    "        trainlist.append(v)\n",
    "traindf=pd.concat(trainlist)\n",
    "validatedf=pd.concat(validatelist)\n",
    "testdf=pd.concat(testlist)\n",
    "print traindf.shape, validatedf.shape, testdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maskval=[e in set(validatedf.business_id).difference(set(traindf.business_id)) for e in validatedf.business_id] \n",
    "masktest=[e in set(testdf.business_id).difference(set(traindf.business_id)) for e in testdf.business_id] \n",
    "print np.sum(maskval), np.sum(masktest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf=recompute_frame(traindf)\n",
    "validatedf=recompute_frame(validatedf)\n",
    "testdf=recompute_frame(testdf)\n",
    "validatedf=validatedf[['user_id', 'business_id','stars', 'review_id']]\n",
    "testdf=testdf[['user_id', 'business_id', 'stars', 'review_id']]\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf.to_csv(\"tempdata/strain.csv\", index=False, header=True, encoding=\"utf-8\")\n",
    "validatedf.to_csv(\"tempdata/svalidate.csv\", index=False, header=True, encoding=\"utf-8\")\n",
    "testdf.to_csv(\"tempdata/stest.csv\", index=False, header=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ybar = traindf.stars.mean()\n",
    "ybar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids=traindf.user_id.unique()#unique-user-ids\n",
    "uiids=traindf.business_id.unique()#unique-item-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results function from pset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_results(stars_actual, stars_predicted, ylow=1, yhigh=6, model=\"\", predicteds=False, onsame=False, axis=False):\n",
    "    \"\"\"\n",
    "    plot predicted results against actual results. Takes 2 arguments: a\n",
    "    numpy array of actual ratings and a numpy array of predicted ratings\n",
    "    scatterplots the predictions, a unit slope line, line segments joining the mean,\n",
    "    and a filled in area of the standard deviations.\"\n",
    "    \"\"\"\n",
    "    if onsame:\n",
    "        ax=onsame\n",
    "    elif axis:\n",
    "        ax=axis\n",
    "    else:\n",
    "        fig=plt.figure()\n",
    "        ax=plt.gca()\n",
    "    df=pd.DataFrame(dict(actual=stars_actual, predicted=stars_predicted))\n",
    "    xp=[]\n",
    "    yp=[]\n",
    "    for k,v in df.groupby('actual'):\n",
    "        xp.append(k)\n",
    "        yp.append(v.predicted.mean())        \n",
    "    cl, = ax.plot(xp,yp, 's-', label=\"means for %s\" % model)\n",
    "    c=cl.get_color()\n",
    "    sig=df.groupby('actual').predicted.std().values\n",
    "    ax.fill_between(xp, yp - sig, yp + sig, color=c, alpha=0.2)\n",
    "    if predicteds:\n",
    "        ax.plot(df.actual, df.predicted, '.', color=c, alpha=0.1, label=\"predicted for %s\" % model)\n",
    "\n",
    "    if not onsame:\n",
    "        ax.plot([1,5],[1,5], 'k', label=\"slope 1\")\n",
    "        ax.set_xlabel(\"actual\")\n",
    "        ax.set_ylabel(\"predicted\")\n",
    "        ax.set_ylim([ylow,yhigh])\n",
    "        ax.set_xlim([0.9, 5.1])\n",
    "    ax.legend(frameon=False, loc=\"upper left\")\n",
    "    rmse=get_rmse(stars_actual, stars_predicted)\n",
    "    print \"RMSE for %s\" % model, rmse\n",
    "    return ax,rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an idea for a function that can be used to determine batter similarity. The function takes the batter-pitcher pair to predict, and it returns similar batters who have faced the given pitcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sim_batters(bID, pID, df):\n",
    "    batter_row = df.loc[df['bID'] == bID]\n",
    "    AVG = batter_row['AVG']\n",
    "    OBP = batter_row['OBP']\n",
    "    if (batter_row['RL'] == 'R'):\n",
    "        hand = 0\n",
    "    else:\n",
    "        hand = 1\n",
    "    SAC = batter_row['SAC']\n",
    "    # continue for other statistics\n",
    "    \n",
    "    # find list of bIDs who have faced given pID\n",
    "    p_batters = df.loc[df['pID'] == pID]\n",
    "    \n",
    "    similarity = {}\n",
    "    # for each batter, calculate similarity\n",
    "    for b in p_batters:\n",
    "        comp_b_row = df.loc[df['bID'] == b]\n",
    "        comp_AVG = batter_row['AVG']\n",
    "        comp_OBP = batter_row['OBP']\n",
    "        if (batter_row['RL'] == 'R'):\n",
    "            comp_hand = 0\n",
    "        else:\n",
    "            comp_hand = 1\n",
    "        comp_SAC = batter_row['SAC']\n",
    "        # continue for other statistics\n",
    "        \n",
    "        #calculate similarity\n",
    "        AVG_score = AVG_weight*(AVG - comp_AVG)/AVG\n",
    "        OBP_score = OBP_weight*(OBP - comp_OPB)/OBP\n",
    "        if (hand == comp_hand):\n",
    "            hand_score = hand_weight\n",
    "        else:\n",
    "             hand_score = 0   \n",
    "        SAC_score = SAC_weight*(SAC - comp_SAC)/SAC\n",
    "        total_score = AVG_score + OBP_score + hand_score + SAC_score\n",
    "        similarity[b] = total_score\n",
    "    # return top x bIDs with lowest scores\n",
    "    sorted_sim = sorted(similarity.items(), key=operator.itemgetter(1))\n",
    "    sim_batters = sorted_sim.keys\n",
    "    return sim_batters[:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are functions from pset 4 that might be useful for the KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "# calculate pearson sim between 2 pitchers\n",
    "def pearson_sim(pitcher1, pitcher2, n_common):\n",
    "    p1_avg = pitcher1['ov_AVG']\n",
    "    p2_avg = pitcher2['ov_AVG']\n",
    "    p1_actual = pitcher1['AVG']\n",
    "    p2_actual = pitcher2['AVG']\n",
    "    norm1 = []\n",
    "    norm2 = []\n",
    "    for i in range(len(rest1)):\n",
    "        norm1.append(rest1[i] - p1_actual[i])\n",
    "        norm2.append(rest2[i] - p2_actual[i])\n",
    "    if (n_common == 0) or (n_common==1):\n",
    "        rho = 0;\n",
    "    else:\n",
    "        rho = sp.stats.pearsonr(norm1,norm2)[0]\n",
    "    if np.isnan(rho):\n",
    "        return 0;\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_restaurant_reviews(pID, df, set_of_batters):\n",
    "    \"\"\"\n",
    "    given a pitcher id and a set of batters, return the sub-dataframe of their\n",
    "    averages.\n",
    "    \"\"\"\n",
    "    mask = (df.user_id.isin(set_of_batters)) & (df.pID==pID)\n",
    "    avgs = df[mask]\n",
    "    avgs = avgs[avgs.user_id.duplicated()==False]\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Database:\n",
    "    \"A class representing a database of similarities and common supports\"\n",
    "    \n",
    "    def __init__(self, rindexmap, supports):\n",
    "        \"the constructor, takes a map of restaurant id's to integers\"\n",
    "        database={}\n",
    "        self.rindexmap=rindexmap\n",
    "        self.supports=supports\n",
    "        l_keys=len(self.rindexmap.keys())\n",
    "        self.database_sim=np.zeros([l_keys,l_keys])\n",
    "        self.database_sup=np.zeros([l_keys, l_keys], dtype=np.int)\n",
    "\n",
    "    def set_supports(self, supports):\n",
    "        self.supports=supports\n",
    "        \n",
    "    def get(self, b1, b2):\n",
    "        \"returns a tuple of similarity,common_support given two business ids\"\n",
    "        sim=self.database_sim[self.rindexmap[b1]][self.rindexmap[b2]]\n",
    "        nsup=self.database_sup[self.rindexmap[b1]][self.rindexmap[b2]]\n",
    "        return (sim, nsup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uiidmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-cec2aee98e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muiidmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'uiidmap' is not defined"
     ]
    }
   ],
   "source": [
    "db=Database(uiidmap, supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapper1(row):\n",
    "    return row[1], (row[2], row[5], row[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combiner(items):\n",
    "    indict={}\n",
    "    for key, value in items:\n",
    "        if not indict.has_key(key):\n",
    "            indict[key]=[]\n",
    "        indict[key].append(value)\n",
    "    return indict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reducer1(the_input):\n",
    "    bID, values = the_input\n",
    "    avgs=[]\n",
    "    for pID,AVG,ov_AVG in values:\n",
    "        avgs.append((pID,(AVG, ov_AVG)))\n",
    "    return bID, avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "def mapper2(list_input):\n",
    "    nlist = []\n",
    "    comb = list(combinations_with_replacement(list_input[1], 2))\n",
    "    for item in comb:\n",
    "        if item[0][0] > item[1][0]:\n",
    "            biz_pair = item[1][0], item[0][0]\n",
    "            star_pair = item[1][1], item[0][1]\n",
    "        else:\n",
    "            biz_pair = item[0][0], item[1][0]\n",
    "            star_pair = item[0][1], item[1][1]\n",
    "        tup = (biz_pair, star_pair)\n",
    "        nlist.append(tup)\n",
    "    return nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combiner_list(itemslist):\n",
    "    indict={}\n",
    "    for items in itemslist:\n",
    "        for key, value in items:\n",
    "            if not indict.has_key(key):\n",
    "                indict[key]=[]\n",
    "            indict[key].append(value)\n",
    "    return indict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reducer2(item_dict):\n",
    "    p1_id = item_dict[0][0]\n",
    "    p2_id = item_dict[0][1]\n",
    "    AVG_1 = [x[0][0] for x in item_dict[1]]\n",
    "    AVG_2 = [x[1][0] for x in item_dict[1]]\n",
    "    ov_AVG_1 = [x[0][1] for x in item_dict[1]]\n",
    "    ov_AVG_2 = [x[1][1] for x in item_dict[1]]\n",
    "    n_common = len(item_dict[1])\n",
    "    p1_dict = {'ov_AVG': ov_AVG_1, 'AVG': AVG_1}\n",
    "    p2_dict = {'ov_AVG': ov_AVG_2, 'AVG': AVG_2}\n",
    "    rho = pearson_sim(p1_dict, p2_dict, n_common)\n",
    "    return (p1_id, p2_id),(rho, n_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_reduce(tuples):\n",
    "    mapped1=map(mapper1, tuples)\n",
    "    combine1=combiner(mapped1)\n",
    "    reduced1=reduce(lambda x,y: x + [reducer1(y)], combine1, [])\n",
    "    mapped2=map(mapper2,reduced1)\n",
    "    combine2=combiner_list(mapped2)\n",
    "    output=reduce(lambda x,y: x + [reducer2(y)], combine2, [])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-05bad35a5c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraindf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-5391d317792c>\u001b[0m in \u001b[0;36mmap_reduce\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcombine1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombiner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreduced1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreducer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmapped2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduced1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcombine2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombiner_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreducer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-942c0d203844>\u001b[0m in \u001b[0;36mmapper2\u001b[0;34m(list_input)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mbiz_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mstar_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mbiz_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuples=traindf.itertuples()\n",
    "sims=map_reduce(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_from_mr(db, df, mapredlist):\n",
    "    for tpair,vpair in mapredlist:\n",
    "        i1=db.rindexmap[tpair[0]]\n",
    "        i2=db.rindexmap[tpair[1]]\n",
    "        db.database_sim[i1][i2]=vpair[0]\n",
    "        db.database_sup[i1][i2]=vpair[1]\n",
    "        db.database_sim[i2][i1]=vpair[0]\n",
    "        db.database_sup[i2][i1]=vpair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "populate_from_mr(db, traindf, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking work with populate_by_calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db2=Database( uiidmap, supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(db, df, p1, p2, similarity_func):\n",
    "    # find common reviewers\n",
    "    common_reviewers = db.supports[db.rindexmap[p1]][db.rindexmap[p2]]\n",
    "    n_common=len(common_reviewers)\n",
    "    if p1==p2:\n",
    "        return 1., n_common\n",
    "    #get reviews\n",
    "    p1_ov_AVG = get_restaurant_reviews(p1, df, common_reviewers)\n",
    "    p2_ov_AVG = get_restaurant_reviews(p2, df, common_reviewers)\n",
    "    sim=similarity_func(p1_ov_AVG, p2_ov_AVG, n_common)\n",
    "    return sim, n_common\n",
    "\n",
    "def populate_by_calculating(db, df, similarity_func):\n",
    "    \"\"\"\n",
    "    a populator for every pair of businesses in df. takes similarity_func like\n",
    "    pearson_sim as argument\n",
    "    \"\"\"\n",
    "    items=db.rindexmap.items()\n",
    "    for b1, i1 in items:\n",
    "        for b2, i2 in items:\n",
    "            if i1 <= i2:\n",
    "                sim, nsup=calculate_similarity(db, df, b1, b2, similarity_func)\n",
    "                db.database_sim[i1][i2]=sim\n",
    "                db.database_sim[i2][i1]=sim\n",
    "                db.database_sup[i1][i2]=nsup\n",
    "                db.database_sup[i2][i1]=nsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "populate_by_calculating(db2, traindf, pearson_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpair=('FV0BkoGOd3Yu_eJnXY15ZA', 'O-Xa9GCFWI65YiBD5Jw_hA')\n",
    "print db.get(tpair[0],tpair[1]),db2.get(tpair[0],tpair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shrunk_sim(sim, n_common, reg=3.):\n",
    "    \"takes a similarity and shrinks it down by using the regularizer\"\n",
    "    ssim=(n_common*sim)/(n_common+reg)\n",
    "    return ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "knearest\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "restaurant_id : string\n",
    "    The id of the restaurant whose nearest neighbors we want\n",
    "set_of_restaurants : array\n",
    "    The set of restaurants from which we want to find the nearest neighbors\n",
    "dbase : instance of Database class.\n",
    "    A database of similarities, on which the get method can be used to get the similarity\n",
    "  of two businesses. e.g. dbase.get(rid1,rid2)\n",
    "k : int\n",
    "    the number of nearest neighbors desired, default 7\n",
    "reg: float\n",
    "    the regularization.\n",
    "    \n",
    "  \n",
    "Returns\n",
    "--------\n",
    "A sorted list\n",
    "    of the top k similar restaurants. The list is a list of tuples\n",
    "    (business_id, shrunken similarity, common support).\n",
    "\"\"\"\n",
    "from operator import itemgetter\n",
    "def knearest(restaurant_id, set_of_restaurants, dbase, k=7, reg=3.):\n",
    "    \"\"\"\n",
    "    Given a restaurant_id, dataframe, and database, get a sorted list of the\n",
    "    k most similar restaurants from the set of restaurants.\n",
    "    \"\"\"\n",
    "    similars=[]\n",
    "    for other_rest_id in set_of_restaurants:\n",
    "        if other_rest_id!=restaurant_id:\n",
    "            sim, nc=dbase.get(restaurant_id, other_rest_id)\n",
    "            ssim=shrunk_sim(sim, nc, reg=reg)\n",
    "            simdist=(1. - ssim)/2.\n",
    "            similars.append((other_rest_id, simdist, nc ))\n",
    "    similars=sorted(similars, key=itemgetter(1))\n",
    "    return similars[0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_users_restaurants(df, user_id):\n",
    "    dfuser=df[df.user_id==user_id]\n",
    "    dfuserdedup=dfuser.drop_duplicates('business_id')\n",
    "    return dict(zip(dfuserdedup.business_id.values, dfuserdedup.stars.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "rating\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "set_of_restaurants: Dictionary\n",
    "    The dictionary of restaurant: star-rating pairs you want to make the prediction from.\n",
    "    This would be the output of a function like get_users_restaurants\n",
    "train_map: Dictionary\n",
    "    A dictionary with keys mean, users and items which have estimates of\n",
    "    overall average or intercept, user coefficients(averages), and\n",
    "    item coefficients(averages) respectively\n",
    "dbase : instance of Database class.\n",
    "    A database of similarities, on which the get method can be used to get the similarity\n",
    "  of two businessed. e.g. dbase.get(rid1,rid2)\n",
    "restaurant_id : string\n",
    "    The id of the restaurant whose nearest neighbors we want\n",
    "user_id : string\n",
    "    The id of the user, in whose reviewed restaurants we want to find the neighbors\n",
    "k : int\n",
    "    the number of nearest neighbors desired, default 7\n",
    "reg: float\n",
    "    the regularization.\n",
    "    \n",
    "  \n",
    "Returns\n",
    "--------\n",
    "A float\n",
    "    which is the imputed rating that we predict that user_id will make for restaurant_id\n",
    "    \n",
    "Notes\n",
    "--------\n",
    "If the sum of scores is 0, return the baseline estimate of the ranking.\n",
    "\"\"\"\n",
    "#your code here\n",
    "# Note: this function was inspired in part by the solutions to the 2013 hw4\n",
    "def rating(set_of_restaurants, train_map, dbase, restaurant_id, user_id, k=7, reg=3.):\n",
    "    mu=train_map['mean']\n",
    "    user_bias = train_map['users'][user_id]\n",
    "    nsum=0.\n",
    "    scoresum=0.\n",
    "    nears=knearest(restaurant_id, set_of_restaurants, dbase, k=k, reg=reg)\n",
    "    restaurant_bias=train_map['items'][restaurant_id]\n",
    "    scores=[]\n",
    "    for r,s,nc in nears:\n",
    "        ssim = 1-s\n",
    "        scoresum=scoresum+ssim\n",
    "        scores.append(ssim)\n",
    "        r_biases = train_map['items'][r]\n",
    "        r_stars = set_of_restaurants[r]\n",
    "        rminusb=(r_stars - (r_biases + user_bias + mu))\n",
    "        nsum=nsum+ssim*rminusb\n",
    "    baseline=(user_bias +restaurant_bias + mu)\n",
    "    if scoresum > 0.:\n",
    "        val =  nsum/scoresum + baseline\n",
    "    else:\n",
    "        val=baseline\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainuser=traindf.loc[0].user_id\n",
    "testrest=testdf[testdf.user_id==trainuser].business_id.values[0]\n",
    "print trainuser, testrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actual(df, userid, bizid):\n",
    "    return df[(df.user_id==userid) & (df.business_id==bizid)]['stars'].values[0]\n",
    "\n",
    "print \"Actual\", get_actual(testdf, trainuser, testrest)\n",
    "print \"Predicted\",rating(get_users_restaurants(traindf, trainuser), train_avgs, db, testrest, trainuser, k=2, reg=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratings_user_nbd(indf, traindf, train_map, db, k=2, reg=3.):\n",
    "    zips=zip(indf.business_id, indf.user_id, indf.stars)\n",
    "    preds=[]\n",
    "    actuals=[]\n",
    "    for (r,u,actual) in zips:\n",
    "        pred=rating(get_users_restaurants(traindf, u),train_map, db, r,u, k, reg)\n",
    "        preds.append(pred)\n",
    "        actuals.append(actual)\n",
    "    return np.array(preds), np.array(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt, at = get_ratings_user_nbd(traindf, traindf, train_avgs, db, k=4, reg=4.)\n",
    "compare_results(at,pt, model=\"knn(user) on training k=4, reg=4\", predicteds=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
